{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !unzip /content/drive/MyDrive/gender_dataset_face-20220319T023558Z-001.zip -d /content/drive/MyDrive/gender_dataset_face"
      ],
      "metadata": {
        "id": "Mgpci_UotUVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9GNSc9Bntjb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"Model_weights\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "bVVlsIbLVRyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n"
      ],
      "metadata": {
        "id": "47FQgndsUehK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z94GVpdgmbZz"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Flatten\n",
        "\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from keras import backend as K\n",
        "from keras import models\n",
        "from keras.applications.vgg16 import VGG16\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VymXLjYoeDW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLwedq3AmoSt"
      },
      "outputs": [],
      "source": [
        "class SmallerVGGNet:\n",
        "    @staticmethod\n",
        "    def vgg_net(width, height, depth, classes):\n",
        "\n",
        "            conv_base = VGG16(weights='imagenet',\n",
        "                          include_top=False,\n",
        "                          input_shape=(height, width, depth))\n",
        "            model = models.Sequential()\n",
        "\n",
        "            model.add(conv_base)\n",
        "            model.add(Flatten())\n",
        "\n",
        "            model.add(Dense(1024, activation='relu'))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dropout(0.5))\n",
        "            model.add(Dense(classes, activation='sigmoid'))\n",
        "            return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnSXYlVgoEbN"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")  # Only needed in headless environments like servers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.utils import img_to_array, to_categorical, plot_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# If you have a custom model, make sure the import is correct\n",
        "# from model.smallervggnet import SmallerVGGNet  # Uncomment if you have this file and model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "class SmallerVGGNet:\n",
        "    @staticmethod\n",
        "    def build(width, height, depth, classes):\n",
        "        model = Sequential()\n",
        "\n",
        "        # First CONV => RELU => POOL layer\n",
        "        model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", input_shape=(height, width, depth)))\n",
        "        model.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "\n",
        "        # Second CONV => RELU => POOL layer\n",
        "        model.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "        # Third CONV => RELU => POOL layer\n",
        "        model.add(Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "        # Flatten and add Fully Connected layers\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(256, activation=\"relu\"))\n",
        "        model.add(Dropout(0.5))  # Prevent overfitting\n",
        "        model.add(Dense(classes, activation=\"softmax\"))  # Output layer\n",
        "\n",
        "        return model\n",
        "\n",
        "model = SmallerVGGNet.build(width=img_dims[0], height=img_dims[1], depth=img_dims[2], classes=2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b6Qp_KIiWp-u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60bb3763-860d-4922-acbc-64803f8762d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wKk0N5WnXDk",
        "outputId": "52b6fb8c-ca58-45fc-cc5a-6818be404afc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.5576 - loss: 0.6923\n",
            "Epoch 1: val_accuracy improved from -inf to 0.74831, saving model to epochs:001-val_accuracy:0.748.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 136ms/step - accuracy: 0.5582 - loss: 0.6920 - val_accuracy: 0.7483 - val_loss: 0.5457\n",
            "Epoch 2/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8125 - loss: 0.5345\n",
            "Epoch 2: val_accuracy did not improve from 0.74831\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8125 - loss: 0.5345 - val_accuracy: 0.7104 - val_loss: 0.5609\n",
            "Epoch 3/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7157 - loss: 0.5623\n",
            "Epoch 3: val_accuracy improved from 0.74831 to 0.84574, saving model to epochs:003-val_accuracy:0.846.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 99ms/step - accuracy: 0.7158 - loss: 0.5621 - val_accuracy: 0.8457 - val_loss: 0.4025\n",
            "Epoch 4/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6875 - loss: 0.5384\n",
            "Epoch 4: val_accuracy did not improve from 0.84574\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6875 - loss: 0.5384 - val_accuracy: 0.8322 - val_loss: 0.4186\n",
            "Epoch 5/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7803 - loss: 0.4730\n",
            "Epoch 5: val_accuracy improved from 0.84574 to 0.85386, saving model to epochs:005-val_accuracy:0.854.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 83ms/step - accuracy: 0.7805 - loss: 0.4728 - val_accuracy: 0.8539 - val_loss: 0.3571\n",
            "Epoch 6/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.2265\n",
            "Epoch 6: val_accuracy did not improve from 0.85386\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.2265 - val_accuracy: 0.8471 - val_loss: 0.3590\n",
            "Epoch 7/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8180 - loss: 0.4307\n",
            "Epoch 7: val_accuracy improved from 0.85386 to 0.86604, saving model to epochs:007-val_accuracy:0.866.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.8181 - loss: 0.4306 - val_accuracy: 0.8660 - val_loss: 0.3287\n",
            "Epoch 8/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7812 - loss: 0.4756\n",
            "Epoch 8: val_accuracy improved from 0.86604 to 0.87145, saving model to epochs:008-val_accuracy:0.871.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7812 - loss: 0.4756 - val_accuracy: 0.8714 - val_loss: 0.3233\n",
            "Epoch 9/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8316 - loss: 0.3892\n",
            "Epoch 9: val_accuracy improved from 0.87145 to 0.87957, saving model to epochs:009-val_accuracy:0.880.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 101ms/step - accuracy: 0.8316 - loss: 0.3892 - val_accuracy: 0.8796 - val_loss: 0.2919\n",
            "Epoch 10/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8438 - loss: 0.3461\n",
            "Epoch 10: val_accuracy improved from 0.87957 to 0.88227, saving model to epochs:010-val_accuracy:0.882.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8438 - loss: 0.3461 - val_accuracy: 0.8823 - val_loss: 0.2808\n",
            "Epoch 11/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.8500 - loss: 0.3643\n",
            "Epoch 11: val_accuracy did not improve from 0.88227\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 80ms/step - accuracy: 0.8500 - loss: 0.3643 - val_accuracy: 0.8796 - val_loss: 0.2845\n",
            "Epoch 12/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8125 - loss: 0.2886\n",
            "Epoch 12: val_accuracy improved from 0.88227 to 0.88633, saving model to epochs:012-val_accuracy:0.886.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8125 - loss: 0.2886 - val_accuracy: 0.8863 - val_loss: 0.2723\n",
            "Epoch 13/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.8664 - loss: 0.3342\n",
            "Epoch 13: val_accuracy improved from 0.88633 to 0.90528, saving model to epochs:013-val_accuracy:0.905.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 81ms/step - accuracy: 0.8663 - loss: 0.3342 - val_accuracy: 0.9053 - val_loss: 0.2388\n",
            "Epoch 14/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8750 - loss: 0.2818\n",
            "Epoch 14: val_accuracy did not improve from 0.90528\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8750 - loss: 0.2818 - val_accuracy: 0.9053 - val_loss: 0.2345\n",
            "Epoch 15/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8660 - loss: 0.3259\n",
            "Epoch 15: val_accuracy did not improve from 0.90528\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 101ms/step - accuracy: 0.8660 - loss: 0.3259 - val_accuracy: 0.9039 - val_loss: 0.2461\n",
            "Epoch 16/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8750 - loss: 0.3571\n",
            "Epoch 16: val_accuracy did not improve from 0.90528\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8750 - loss: 0.3571 - val_accuracy: 0.8985 - val_loss: 0.2486\n",
            "Epoch 17/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8686 - loss: 0.3002\n",
            "Epoch 17: val_accuracy did not improve from 0.90528\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 87ms/step - accuracy: 0.8687 - loss: 0.3001 - val_accuracy: 0.8999 - val_loss: 0.2158\n",
            "Epoch 18/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.1908\n",
            "Epoch 18: val_accuracy improved from 0.90528 to 0.90934, saving model to epochs:018-val_accuracy:0.909.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9688 - loss: 0.1908 - val_accuracy: 0.9093 - val_loss: 0.2217\n",
            "Epoch 19/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.8921 - loss: 0.2736\n",
            "Epoch 19: val_accuracy improved from 0.90934 to 0.91204, saving model to epochs:019-val_accuracy:0.912.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - accuracy: 0.8921 - loss: 0.2736 - val_accuracy: 0.9120 - val_loss: 0.2183\n",
            "Epoch 20/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9375 - loss: 0.1657\n",
            "Epoch 20: val_accuracy did not improve from 0.91204\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.1657 - val_accuracy: 0.9120 - val_loss: 0.2168\n",
            "Epoch 21/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8835 - loss: 0.2721\n",
            "Epoch 21: val_accuracy did not improve from 0.91204\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 103ms/step - accuracy: 0.8835 - loss: 0.2721 - val_accuracy: 0.8823 - val_loss: 0.2536\n",
            "Epoch 22/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8750 - loss: 0.3000\n",
            "Epoch 22: val_accuracy did not improve from 0.91204\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8750 - loss: 0.3000 - val_accuracy: 0.8890 - val_loss: 0.2404\n",
            "Epoch 23/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9185 - loss: 0.2150\n",
            "Epoch 23: val_accuracy improved from 0.91204 to 0.92558, saving model to epochs:023-val_accuracy:0.926.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 92ms/step - accuracy: 0.9183 - loss: 0.2153 - val_accuracy: 0.9256 - val_loss: 0.1860\n",
            "Epoch 24/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8750 - loss: 0.2739\n",
            "Epoch 24: val_accuracy did not improve from 0.92558\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8750 - loss: 0.2739 - val_accuracy: 0.9242 - val_loss: 0.1790\n",
            "Epoch 25/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9104 - loss: 0.2249\n",
            "Epoch 25: val_accuracy did not improve from 0.92558\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - accuracy: 0.9104 - loss: 0.2250 - val_accuracy: 0.9215 - val_loss: 0.1880\n",
            "Epoch 26/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9375 - loss: 0.2087\n",
            "Epoch 26: val_accuracy did not improve from 0.92558\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.2087 - val_accuracy: 0.9229 - val_loss: 0.1857\n",
            "Epoch 27/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9090 - loss: 0.2175\n",
            "Epoch 27: val_accuracy did not improve from 0.92558\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 107ms/step - accuracy: 0.9090 - loss: 0.2176 - val_accuracy: 0.9188 - val_loss: 0.1652\n",
            "Epoch 28/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9375 - loss: 0.1691\n",
            "Epoch 28: val_accuracy did not improve from 0.92558\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.1691 - val_accuracy: 0.9161 - val_loss: 0.1801\n",
            "Epoch 29/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9074 - loss: 0.2186\n",
            "Epoch 29: val_accuracy improved from 0.92558 to 0.94046, saving model to epochs:029-val_accuracy:0.940.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - accuracy: 0.9074 - loss: 0.2186 - val_accuracy: 0.9405 - val_loss: 0.1574\n",
            "Epoch 30/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9375 - loss: 0.1996\n",
            "Epoch 30: val_accuracy did not improve from 0.94046\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.1996 - val_accuracy: 0.9391 - val_loss: 0.1619\n",
            "Epoch 31/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9150 - loss: 0.2059\n",
            "Epoch 31: val_accuracy did not improve from 0.94046\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - accuracy: 0.9150 - loss: 0.2059 - val_accuracy: 0.9405 - val_loss: 0.1597\n",
            "Epoch 32/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9062 - loss: 0.1882\n",
            "Epoch 32: val_accuracy improved from 0.94046 to 0.94317, saving model to epochs:032-val_accuracy:0.943.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9062 - loss: 0.1882 - val_accuracy: 0.9432 - val_loss: 0.1555\n",
            "Epoch 33/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8985 - loss: 0.2451\n",
            "Epoch 33: val_accuracy did not improve from 0.94317\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 89ms/step - accuracy: 0.8987 - loss: 0.2448 - val_accuracy: 0.9202 - val_loss: 0.1891\n",
            "Epoch 34/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9375 - loss: 0.1261\n",
            "Epoch 34: val_accuracy did not improve from 0.94317\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.1261 - val_accuracy: 0.9215 - val_loss: 0.1846\n",
            "Epoch 35/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9195 - loss: 0.2025\n",
            "Epoch 35: val_accuracy did not improve from 0.94317\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 99ms/step - accuracy: 0.9195 - loss: 0.2026 - val_accuracy: 0.9337 - val_loss: 0.1730\n",
            "Epoch 36/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9375 - loss: 0.1752\n",
            "Epoch 36: val_accuracy did not improve from 0.94317\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.1752 - val_accuracy: 0.9323 - val_loss: 0.1700\n",
            "Epoch 37/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9301 - loss: 0.1756\n",
            "Epoch 37: val_accuracy did not improve from 0.94317\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 80ms/step - accuracy: 0.9300 - loss: 0.1758 - val_accuracy: 0.9188 - val_loss: 0.1899\n",
            "Epoch 38/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8438 - loss: 0.2620\n",
            "Epoch 38: val_accuracy did not improve from 0.94317\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8438 - loss: 0.2620 - val_accuracy: 0.9229 - val_loss: 0.1785\n",
            "Epoch 39/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.9205 - loss: 0.1894\n",
            "Epoch 39: val_accuracy did not improve from 0.94317\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 101ms/step - accuracy: 0.9206 - loss: 0.1893 - val_accuracy: 0.9337 - val_loss: 0.1812\n",
            "Epoch 40/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9000 - loss: 0.1054\n",
            "Epoch 40: val_accuracy did not improve from 0.94317\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9000 - loss: 0.1054 - val_accuracy: 0.9310 - val_loss: 0.1786\n",
            "Epoch 41/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9243 - loss: 0.1793\n",
            "Epoch 41: val_accuracy did not improve from 0.94317\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 97ms/step - accuracy: 0.9243 - loss: 0.1794 - val_accuracy: 0.9350 - val_loss: 0.1697\n",
            "Epoch 42/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8750 - loss: 0.2290\n",
            "Epoch 42: val_accuracy did not improve from 0.94317\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8750 - loss: 0.2290 - val_accuracy: 0.9242 - val_loss: 0.1961\n",
            "Epoch 43/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9258 - loss: 0.1902\n",
            "Epoch 43: val_accuracy did not improve from 0.94317\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 79ms/step - accuracy: 0.9258 - loss: 0.1902 - val_accuracy: 0.9337 - val_loss: 0.1979\n",
            "Epoch 44/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0636\n",
            "Epoch 44: val_accuracy did not improve from 0.94317\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0636 - val_accuracy: 0.9350 - val_loss: 0.1982\n",
            "Epoch 45/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9358 - loss: 0.1780\n",
            "Epoch 45: val_accuracy did not improve from 0.94317\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 97ms/step - accuracy: 0.9357 - loss: 0.1782 - val_accuracy: 0.9378 - val_loss: 0.1524\n",
            "Epoch 46/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9375 - loss: 0.1278\n",
            "Epoch 46: val_accuracy did not improve from 0.94317\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.1278 - val_accuracy: 0.9337 - val_loss: 0.1555\n",
            "Epoch 47/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.9304 - loss: 0.1788\n",
            "Epoch 47: val_accuracy did not improve from 0.94317\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 97ms/step - accuracy: 0.9305 - loss: 0.1787 - val_accuracy: 0.9364 - val_loss: 0.1597\n",
            "Epoch 48/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8438 - loss: 0.3173\n",
            "Epoch 48: val_accuracy did not improve from 0.94317\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8438 - loss: 0.3173 - val_accuracy: 0.9378 - val_loss: 0.1563\n",
            "Epoch 49/50\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.9275 - loss: 0.1671\n",
            "Epoch 49: val_accuracy improved from 0.94317 to 0.94993, saving model to epochs:049-val_accuracy:0.950.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 138ms/step - accuracy: 0.9276 - loss: 0.1670 - val_accuracy: 0.9499 - val_loss: 0.1304\n",
            "Epoch 50/50\n",
            "\u001b[1m 1/92\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9062 - loss: 0.1510\n",
            "Epoch 50: val_accuracy did not improve from 0.94993\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9062 - loss: 0.1510 - val_accuracy: 0.9486 - val_loss: 0.1295\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "#from SmallerVGGNet import SmallerVGGNet\n",
        "\n",
        "epochs = 50\n",
        "lr = 1e-3\n",
        "batch_size = 32\n",
        "img_dims = (96, 96, 3)\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# Load image files\n",
        "image_files = [f for f in glob.glob('/content/drive/MyDrive/ARCHIVE/gender_dataset_face' + \"/**/*\", recursive=True)\n",
        "               if not os.path.isdir(f)]\n",
        "random.seed(42)\n",
        "random.shuffle(image_files)\n",
        "\n",
        "# Debug: Check if images are found\n",
        "if len(image_files) == 0:\n",
        "    raise ValueError(\"No images found! Check dataset path.\")\n",
        "\n",
        "# Process images\n",
        "for img in image_files:\n",
        "    image = cv2.imread(img)\n",
        "    if image is None:\n",
        "        print(f\"Skipping unreadable image: {img}\")\n",
        "        continue  # Skip corrupted images\n",
        "\n",
        "    image = cv2.resize(image, (96, 96))\n",
        "    image = img_to_array(image)\n",
        "    data.append(image)\n",
        "\n",
        "    label = os.path.basename(os.path.dirname(img))  # Extract folder name\n",
        "    label = 1 if label.lower() == \"woman\" else 0\n",
        "    labels.append([label])\n",
        "\n",
        "# Check dataset size before splitting\n",
        "if len(data) == 0:\n",
        "    raise ValueError(\"No valid images loaded! Check dataset.\")\n",
        "\n",
        "# Preprocessing\n",
        "data = np.array(data, dtype=\"float32\") / 255.0\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Train-test split\n",
        "trainX, testX, trainY, testY = train_test_split(data, labels, test_size=0.2, random_state=2)\n",
        "trainY = to_categorical(trainY, num_classes=2)\n",
        "testY = to_categorical(testY, num_classes=2)\n",
        "\n",
        "# Data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
        "                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
        "                         horizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "# Build and compile model\n",
        "model = SmallerVGGNet.build(width=img_dims[0], height=img_dims[1], depth=img_dims[2], classes=2)\n",
        "opt = Adam(learning_rate=lr, decay=lr / epochs)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "# Model checkpoint\n",
        "filepath = \"epochs:{epoch:03d}-val_accuracy:{val_accuracy:.3f}.keras\"  # Use .keras instead of .hdf5\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,\n",
        "                             save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "\n",
        "# Train model\n",
        "history = model.fit(aug.flow(trainX, trainY, batch_size=batch_size),\n",
        "                    validation_data=(testX, testY),\n",
        "                    steps_per_epoch=len(trainX) // batch_size,\n",
        "                    epochs=epochs, verbose=1, callbacks=callbacks_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lf0hTbYUgVY"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')   if drive is not mounteded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHoykW2tnn3G"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure training history exists before plotting\n",
        "if history and hasattr(history, 'history'):\n",
        "    # Check available keys\n",
        "    keys = history.history.keys()\n",
        "\n",
        "    # Determine correct accuracy keys\n",
        "    acc_key = 'accuracy' if 'accuracy' in keys else 'acc'\n",
        "    val_acc_key = 'val_accuracy' if 'val_accuracy' in keys else 'val_acc'\n",
        "    loss_key = 'loss'\n",
        "    val_loss_key = 'val_loss'\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(history.history[acc_key], label='Train Accuracy')\n",
        "    plt.plot(history.history[val_acc_key], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(history.history[loss_key], label='Train Loss')\n",
        "    plt.plot(history.history[val_loss_key], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Optional: Save the plots\n",
        "    plt.savefig('/content/drive/MyDrive/ARCHIVE/result.png')\n",
        "\n",
        "else:\n",
        "    print(\"Error: Training history is empty or model training was not completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CIJV44syhrv"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymqKlitRpQFt",
        "outputId": "5956d2da-a385-4bc3-d1fb-7cc594ce5c29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "pred = model.predict(testX)\n",
        "pred = np.argmax(pred,axis = 1)\n",
        "y_true = np.argmax(testY,axis = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "63kb2nK_pTUG",
        "outputId": "9a7eeecd-9474-47ef-dbb9-547496c49fc1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "CM = confusion_matrix(y_true, pred)\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "fig, ax = plot_confusion_matrix(conf_mat=CM ,  figsize=(5, 5))\n",
        "# plt.show()\n",
        "fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVZvSg1xpayp",
        "outputId": "81fedc66-c5b8-4421-afe2-316b3c3697ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cvlib in /usr/local/lib/python3.11/dist-packages (0.2.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from cvlib) (1.26.4)\n",
            "Requirement already satisfied: progressbar in /usr/local/lib/python3.11/dist-packages (from cvlib) (2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from cvlib) (2.32.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from cvlib) (11.1.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from cvlib) (2.37.0)\n",
            "Requirement already satisfied: imutils in /usr/local/lib/python3.11/dist-packages (from cvlib) (0.5.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->cvlib) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->cvlib) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->cvlib) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->cvlib) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install cvlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3pmgQeypXct",
        "outputId": "86201671-6d47-4f03-b28d-35cb9f2bdd21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cvlib in /usr/local/lib/python3.11/dist-packages (0.2.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from cvlib) (1.26.4)\n",
            "Requirement already satisfied: progressbar in /usr/local/lib/python3.11/dist-packages (from cvlib) (2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from cvlib) (2.32.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from cvlib) (11.1.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from cvlib) (2.37.0)\n",
            "Requirement already satisfied: imutils in /usr/local/lib/python3.11/dist-packages (from cvlib) (0.5.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->cvlib) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->cvlib) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->cvlib) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->cvlib) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "# from keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "\n",
        "from keras.models import load_model\n",
        "# from keras.utils import get_file\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "import os\n",
        "!pip install cvlib\n",
        "import cvlib as cv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg_k_c-GI9Mi",
        "outputId": "2c088692-1199-417a-a150-799e224bd287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k8_2RNyrDtb"
      },
      "outputs": [],
      "source": [
        "img= '/content/drive/MyDrive/ARCHIVE/man1.jpg'\n",
        "image = cv2.imread(img)\n",
        "# cv2.imread('/conte')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBpE52ruM1No",
        "outputId": "c80daf7a-1ac6-4787-b071-43a8a0fd8804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 8 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7d9164281bc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303ms/step\n",
            "[2.3005035e-07 9.9999976e-01]\n",
            "['man', 'woman']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "image = cv2.imread('/content/drive/MyDrive/ARCHIVE/women2.jpeg')  #give any image from computer\n",
        "\n",
        "if image is None:\n",
        "    print(\"Could not read input image\")\n",
        "    exit()\n",
        "\n",
        "# load pre-trained model\n",
        "model = load_model(\"/content/drive/MyDrive/ARCHIVE/epochs_049-val_accuracy_0.950.keras\")   # check most accuracy and loss and give path of file\n",
        "\n",
        "# detect faces in the image\n",
        "face, confidence = cv.detect_face(image)\n",
        "\n",
        "classes = ['man','woman']\n",
        "# loop through detected faces\n",
        "for idx, f in enumerate(face):\n",
        "\n",
        "     # get corner points of face rectangle\n",
        "    (startX, startY) = f[0], f[1]\n",
        "    (endX, endY) = f[2], f[3]\n",
        "\n",
        "    # draw rectangle over face\n",
        "    cv2.rectangle(image, (startX,startY), (endX,endY), (0,255,0), 2)\n",
        "\n",
        "    # crop the detected face region\n",
        "    face_crop = np.copy(image[startY:endY,startX:endX])\n",
        "\n",
        "    # preprocessing for gender detection model\n",
        "    face_crop = cv2.resize(face_crop,(96,96))\n",
        "    face_crop = face_crop.astype(\"float\") / 255.0\n",
        "    face_crop = img_to_array(face_crop)\n",
        "    face_crop = np.expand_dims(face_crop, axis=0)\n",
        "\n",
        "    # apply gender detection on face\n",
        "    conf = model.predict(face_crop)[0]\n",
        "    print(conf)\n",
        "    print(classes)\n",
        "\n",
        "    # get label with max accuracy\n",
        "    idx = np.argmax(conf)\n",
        "    label = classes[idx]\n",
        "\n",
        "    label = \"{}: {:.2f}%\".format(label, conf[idx] * 100)\n",
        "\n",
        "    Y = startY - 10 if startY - 10 > 10 else startY + 10\n",
        "\n",
        "    # write label and confidence above face rectangle\n",
        "    if conf[idx] * 100 > 50.0:\n",
        "        cv2.putText(image, label, (startX, Y),  cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.7, (0, 255, 0), 1)\n",
        "# cv2.imwrite('/content/',image)\n",
        "# display output\n",
        "# cv2.imshow(\"gender detection\", image)\n",
        "\n",
        "# # press any key to close window\n",
        "# cv2.waitKey(0)\n",
        "\n",
        "# # save output\n",
        "cv2.imwrite(\"gender_detection.jpg\", image)\n",
        "\n",
        "# # release resources\n",
        "# cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSiZR2x6ql7N"
      },
      "source": [
        "Thank you..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UER7TaOvQw07"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}